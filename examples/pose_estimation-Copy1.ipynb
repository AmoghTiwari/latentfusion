{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.backends import cudnn\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.model_zoo\n",
    "\n",
    "MOPED_PATH = Path('')\n",
    "LINEMOD_PATH = Path('/ssd_scratch/cvit/amoghtiwari/latentfusion/data/lm_format')\n",
    "CHECKPOINT = Path('/ssd_scratch/cvit/amoghtiwari/latentfusion/checkpoints/latentfusion-release.pth')\n",
    "num_input_views = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "\n",
    "##########\n",
    "checkpoint = CHECKPOINT\n",
    "############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m2022-06-21 08:44.57\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mcould not import PCL          \u001b[0m [\u001b[34m\u001b[1mlatentfusion.pointcloud\u001b[0m] \n",
      "\u001b[2m2022-06-21 08:45.02\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mloaded model                  \u001b[0m [\u001b[34m\u001b[1mlatentfusion.recon.inference\u001b[0m] \u001b[36mepoch\u001b[0m=\u001b[35m200\u001b[0m \u001b[36mname\u001b[0m=\u001b[35mshapenet,no_mask_morph,fixed_eqlr,256,mask,depth,in_mask,mask_noise_p=0.25,sm=nearest,fuser=gru-branched_20200509_10h19m10s-branched_20200509_10h42m53s-branched_20200509_10h46m53s-branched_20200509_10h48m49s\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from latentfusion.recon.inference import LatentFusionModel\n",
    "\n",
    "model = LatentFusionModel.from_checkpoint(checkpoint, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding Anaconda lib path:/home/amoghtiwari/miniconda3/envs/latentfusion/lib/\n"
     ]
    }
   ],
   "source": [
    "from latentfusion.recon.inference import Observation\n",
    "from latentfusion.pose import bop\n",
    "from latentfusion.datasets.bop_new import BOPDataset\n",
    "from latentfusion.datasets.realsense import RealsenseDataset\n",
    "import latentfusion.visualization as viz\n",
    "from latentfusion.augment import gan_denormalize\n",
    "from latentfusion import meshutils\n",
    "from latentfusion import augment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LINEMOD\n",
    "\n",
    "Uncomment this and comment out the MOPED cell to use LINEMOD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object_scale 0.0047090290923817325\n"
     ]
    }
   ],
   "source": [
    "object_id = 101\n",
    "frame = 0\n",
    "input_scene_path = LINEMOD_PATH / f'train/{object_id:06d}'\n",
    "target_scene_path = LINEMOD_PATH / f'test/{object_id:06d}'\n",
    "\n",
    "input_dataset = BOPDataset(LINEMOD_PATH, input_scene_path, object_id=object_id, object_scale=None)\n",
    "print('object_scale', input_dataset.object_scale)\n",
    "target_dataset = BOPDataset(LINEMOD_PATH, target_scene_path, object_id=object_id, object_scale=None)\n",
    "object_scale_to_meters = 1.0 / (1000.0 * target_dataset.object_scale)\n",
    "pointcloud = input_dataset.load_pointcloud()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MOPED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# object_id = 'toy_plane'\n",
    "# frame = 100\n",
    "\n",
    "# input_scene_dir = MOPED_PATH / object_id / 'reference'\n",
    "# target_scene_dir = MOPED_PATH / object_id / 'evaluation'\n",
    "\n",
    "# pointcloud_path = input_scene_dir / 'integrated_registered_processed.obj'\n",
    "# obj = meshutils.Object3D(pointcloud_path)\n",
    "# pointcloud = torch.tensor(obj.vertices, dtype=torch.float32)\n",
    "# diameter = obj.bounding_diameter\n",
    "# object_scale = 1.0 / diameter\n",
    "# object_scale_to_meters = 1.0 / object_scale\n",
    "\n",
    "# input_paths = [x for x in input_scene_dir.iterdir() if x.is_dir()]\n",
    "# input_dataset = RealsenseDataset(input_paths,\n",
    "#                                  image_scale=1.0,\n",
    "#                                  object_scale=object_scale,\n",
    "#                                  odometry_type='open3d')\n",
    "# target_paths = sorted([x for x in target_scene_dir.iterdir() if x.is_dir()])\n",
    "# target_dataset = RealsenseDataset(target_paths,\n",
    "#                                   image_scale=1.0,\n",
    "#                                   object_scale=object_scale,\n",
    "#                                   odometry_type='open3d',\n",
    "#                                   use_registration=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Dataset into Observations\n",
    "\n",
    "Here we load the data into the `Observation` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/ssd_scratch/cvit/amoghtiwari/latentfusion/data/lm_format/train/000101/mask_visib/000000_000000.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-53083e19c769>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minput_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mObservation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_evenly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_input_views\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtarget_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mObservation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minput_obs_pp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minput_obs_pp_gt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/latentfusion/latentfusion/observation.py\u001b[0m in \u001b[0;36mfrom_dataset\u001b[0;34m(cls, dataset, inds)\u001b[0m\n\u001b[1;32m     77\u001b[0m         loader = torchutils.IndexedDataLoader(\n\u001b[1;32m     78\u001b[0m             dataset, shuffle=False, indices=inds, batch_size=len(inds), num_workers=0)\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/latentfusion/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    598\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/latentfusion/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    636\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/latentfusion/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/latentfusion/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/latentfusion/latentfusion/datasets/bop_new.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mdepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_depth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepth_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/latentfusion/latentfusion/datasets/bop_new.py\u001b[0m in \u001b[0;36m_load_mask\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0mnew_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/latentfusion/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2842\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2843\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2844\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/ssd_scratch/cvit/amoghtiwari/latentfusion/data/lm_format/train/000101/mask_visib/000000_000000.png'"
     ]
    }
   ],
   "source": [
    "input_obs = Observation.from_dataset(input_dataset, inds=input_dataset.sample_evenly(num_input_views))\n",
    "target_obs = Observation.from_dataset(target_dataset, inds=list(range(len(target_dataset)))[frame:frame+1])\n",
    "\n",
    "input_obs_pp = model.preprocess_observation(input_obs)\n",
    "input_obs_pp_gt = model.preprocess_observation(input_obs)\n",
    "target_obs_pp = model.preprocess_observation(target_obs)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(131)\n",
    "viz.show_batch(gan_denormalize(input_obs_pp.color), nrow=4)\n",
    "plt.subplot(132)\n",
    "viz.show_batch(viz.colorize_depth(input_obs_pp.depth), nrow=4)\n",
    "plt.subplot(133)\n",
    "viz.show_batch(viz.colorize_tensor(input_obs_pp.mask), nrow=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Latent Object\n",
    "\n",
    "This builds the 'latent object', referred to as `z_obj` in this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from latentfusion import three\n",
    "import math\n",
    "\n",
    "with torch.no_grad():\n",
    "    z_obj = model.build_latent_object(input_obs_pp)\n",
    "\n",
    "    # Visualize prediction.\n",
    "    camera = input_obs_pp.camera.clone()\n",
    "    y, z = model.render_latent_object(z_obj, camera.to(device))\n",
    "\n",
    "recon_error = (y['depth'].detach().cpu() - input_obs_pp_gt.depth).abs()\n",
    "print('recon_error', recon_error.mean().item())\n",
    "    \n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(131)\n",
    "viz.show_batch(viz.colorize_depth(input_obs_pp.depth), nrow=4, title='GT Depth')\n",
    "plt.subplot(132)\n",
    "viz.show_batch(viz.colorize_depth(y['depth'].detach().cpu()), nrow=4, title='Predicted Depth')\n",
    "plt.subplot(133)\n",
    "viz.show_batch(viz.colorize_tensor(recon_error), nrow=4, title='Error')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize some novel views\n",
    "\n",
    "This visualizes the object from novel views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from latentfusion.three.orientation import evenly_distributed_quats\n",
    "num_novel = 16\n",
    "camera_ref = input_obs.camera[0]\n",
    "camera_ref = camera_ref.zoom(None, model.camera_dist, model.input_size)\n",
    "quats = evenly_distributed_quats(num_novel, upright=True).to(device)\n",
    "camera_test = camera_ref.clone().repeat(num_novel)\n",
    "camera_test.quaternion = quats\n",
    "\n",
    "with torch.no_grad():\n",
    "    y, _ = model.render_ibr_basic(z_obj, input_obs, camera_test.clone().to(device), apply_mask=True)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(131)\n",
    "viz.show_batch(viz.colorize_depth(y['depth']), nrow=4, title='Novel View Depth')\n",
    "plt.subplot(132)\n",
    "viz.show_batch(gan_denormalize(y['color'].cpu()), nrow=4, title='Novel View Color (IBR)')\n",
    "plt.subplot(133)\n",
    "viz.show_batch(viz.colorize_tensor(y['mask']), nrow=4, title='Novel View Mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coarse Pose Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import latentfusion.pose.estimation as pe\n",
    "from functools import partial\n",
    "\n",
    "estimator = pe.load_from_config('../configs/cross_entropy_latent.toml', model, return_camera_history=False, verbose=False)\n",
    "\n",
    "coarse_camera = estimator.estimate(z_obj, target_obs[0])\n",
    "camera_zoom = coarse_camera.zoom(None, model.camera_dist, model.input_size)\n",
    "\n",
    "# Visualize prediction.\n",
    "pred_y, pred_z = model.render_latent_object(z_obj, camera_zoom.to(device))\n",
    "pred_mask = pred_y['mask'].squeeze(0)\n",
    "pred_depth = pred_y['depth'].squeeze(0)\n",
    "pred_depth = camera_zoom.denormalize_depth(pred_depth) * pred_mask\n",
    "pred_depth, _ = camera_zoom.uncrop(pred_depth)\n",
    "pred_mask, _ = camera_zoom.uncrop(pred_mask)\n",
    "pred_depth = pred_depth.cpu()\n",
    "pred_mask = pred_mask.cpu()\n",
    "\n",
    "plt.figure(1, figsize=(10, 10))\n",
    "plt.subplot(131)\n",
    "viz.show_batch(target_obs.color[0], nrow=2)\n",
    "plt.subplot(132)\n",
    "viz.show_batch(viz.colorize_tensor(target_obs[0].depth.cpu(), cmin=pred_depth.max()-1, cmax=pred_depth.max()), nrow=2)\n",
    "plt.subplot(133)\n",
    "viz.show_batch(viz.colorize_tensor(target_obs.prepare()[0].mask.cpu()), nrow=2)\n",
    "\n",
    "plt.figure(2, figsize=(20, 10))\n",
    "plt.subplot(121)\n",
    "viz.show_batch(viz.colorize_tensor(pred_depth.detach().cpu(), cmin=pred_depth.max()-1, cmax=pred_depth.max()), nrow=4, title=\"Estimated Depth\")\n",
    "\n",
    "plt.subplot(122)\n",
    "viz.show_batch(viz.colorize_tensor((target_obs.prepare()[0].depth - pred_depth.detach().cpu()).abs()), nrow=4, title=\"Estimated Depth L1 Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Pose Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from latentfusion import utils\n",
    "from latentfusion.pose import utils as pose_utils\n",
    "from latentfusion.modules.geometry import Camera\n",
    "\n",
    "sgd_estimator = pe.load_from_config('../configs/adam_quick.toml', model, track_stats=True, return_camera_history=True, num_iters=100)\n",
    "\n",
    "init_camera = coarse_camera.clone()\n",
    "refined_camera, stat_history, camera_history = sgd_estimator.estimate(z_obj, target_obs[0], camera=init_camera)\n",
    "camera_zoom = refined_camera.zoom(None, model.camera_dist, model.input_size)\n",
    "\n",
    "# Visualize prediction.\n",
    "pred_y, pred_z = model.render_latent_object(z_obj, camera_zoom.to(device))\n",
    "pred_mask = pred_y['mask'].squeeze(0)\n",
    "pred_depth = pred_y['depth'].squeeze(0)\n",
    "pred_depth = camera_zoom.denormalize_depth(pred_depth) * (pred_y['mask'].squeeze(0) > 0.5)\n",
    "pred_depth, _ = camera_zoom.uncrop(pred_depth)\n",
    "pred_mask, _ = camera_zoom.uncrop(pred_mask)\n",
    "depth_error = (target_obs.prepare()[0].depth - pred_depth.detach().cpu()).abs()\n",
    "pred_depth = pred_depth.cpu()\n",
    "pred_mask = pred_mask.cpu()\n",
    "depth_error = depth_error.cpu()\n",
    "\n",
    "\n",
    "plt.figure(1, figsize=(10, 10))\n",
    "plt.subplot(131)\n",
    "viz.show_batch(target_obs.color[0], nrow=2)\n",
    "plt.subplot(132)\n",
    "viz.show_batch(viz.colorize_tensor(target_obs[0].depth, cmin=pred_depth.max()-1, cmax=pred_depth.max()), nrow=2)\n",
    "plt.subplot(133)\n",
    "viz.show_batch(viz.colorize_tensor(target_obs.prepare()[0].mask), nrow=2)\n",
    "\n",
    "plt.figure(2, figsize=(20, 10))\n",
    "plt.subplot(121)\n",
    "viz.show_batch(viz.colorize_tensor(pred_depth.detach().cpu(), cmin=pred_depth.max()-1, cmax=pred_depth.max()), nrow=4, title=\"Estimated Depth\")\n",
    "\n",
    "plt.subplot(122)\n",
    "viz.show_batch(viz.colorize_tensor((target_obs.prepare()[0].depth - pred_depth.detach().cpu()).abs()), nrow=4, title=\"Estimated Depth L1 Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from latentfusion.pose.metrics import camera_metrics\n",
    "from latentfusion.pose.format import metrics_table_multiple\n",
    "\n",
    "\n",
    "for j in range(1):\n",
    "    print(metrics_table_multiple([\n",
    "            camera_metrics(target_obs.camera, coarse_camera[j], pointcloud, object_scale_to_meters),\n",
    "            camera_metrics(target_obs.camera, refined_camera[j], pointcloud, object_scale_to_meters),\n",
    "    ], ['Coarse', 'Refined'], tablefmt='simple'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot pose refinement stats over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "viz.plot_grid(3, figsize=(30, 15), plots=[\n",
    "    viz.Plot('Angular Error', [stat_history['angle_dist']/math.pi*180], \n",
    "             params={'ylabel': 'Error (deg)', 'xlabel': 'Iteration'}),\n",
    "    viz.Plot('Translation Error', [stat_history['trans_dist']*object_scale_to_meters], \n",
    "             params={'ylabel': 'Error (m)', 'xlabel': 'Iteration'}),\n",
    "    viz.Plot('Rank Loss', [stat_history['rank_loss']], \n",
    "             params={'ylabel': 'Loss', 'xlabel': 'Iteration'}),\n",
    "    viz.Plot('Optim Loss', [stat_history['optim_loss']], \n",
    "             params={'ylabel': 'Loss', 'xlabel': 'Iteration'}),\n",
    "    \n",
    "    viz.Plot('Depth Loss', [stat_history['depth_loss']], \n",
    "             params={'ylabel': 'Loss', 'xlabel': 'Iteration'}),\n",
    "    viz.Plot('Overlap Depth Loss', [stat_history['ov_depth_loss']], \n",
    "             params={'ylabel': 'Loss', 'xlabel': 'Iteration'}),\n",
    "    viz.Plot('Mask Loss', [stat_history['mask_loss']], \n",
    "             params={'ylabel': 'Loss', 'xlabel': 'Iteration'}),\n",
    "    viz.Plot('IOU Loss', [stat_history['iou_loss']], \n",
    "             params={'ylabel': 'Loss', 'xlabel': 'Iteration'}),\n",
    "    viz.Plot('<5 deg <5 cm', [(stat_history['trans_dist']*object_scale_to_meters < 0.05) & (stat_history['angle_dist']/math.pi*180 < 5)], \n",
    "             params={'ylabel': 'Success', 'xlabel': 'Iteration'}),\n",
    "]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
